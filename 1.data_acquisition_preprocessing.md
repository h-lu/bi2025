// ... existing code ...
**模块一：数据采集与预处理 (Data Acquisition & Preprocessing)**

### **模块概述**

本模块是 “Python 与 AI 驱动的现代商务智能” 课程的第一个模块，重点介绍现代商务智能项目中至关重要的数据采集和预处理环节。  数据是所有数据分析和 AI 应用的基础，高质量的数据是构建有效模型的先决条件。本模块旨在帮助学生掌握从各种数据源获取数据，并对数据进行清洗、转换和整合的关键技能，为后续的数据分析、建模和应用开发奠定坚实的基础。

### **模块目标**

*   了解现代商务智能项目中数据源的多样性，掌握不同数据源的特点和获取方法。
*   掌握使用 Python 进行网络数据采集的基本方法，包括 Requests 库和 Scrapy 框架的应用。
*   掌握动态网页数据抓取技术，能够使用 Playwright 等工具应对反爬虫机制。
*   熟练使用 Pandas 库进行数据清洗、转换、整合和预处理操作。
*   了解大规模数据处理的基本概念和 PySpark 库的应用 (可选)。
*   掌握文本数据的基本处理方法，包括 BeautifulSoup 和正则表达式的应用。
*   培养数据质量意识，理解数据预处理在数据分析和建模中的重要性。

### **模块主题**

#### **1. 数据源多样性**

*   **知识点:**
    *   **结构化数据, 半结构化数据, 非结构化数据:**  理解不同数据类型的特点和适用场景。
    *   **内部数据源 vs. 外部数据源:**  区分企业内部和外部数据来源，例如 CRM 系统数据、运营数据 vs. 互联网数据、第三方数据。
    *   **常见数据源类型:**
        *   **Web APIs (应用程序编程接口):**  了解 RESTful API 的概念，掌握使用 API 获取数据的基本方法 (例如，天气 API, 股票 API, 社交媒体 API)。
        *   **数据库:**  关系型数据库 (SQL, 例如 MySQL, PostgreSQL) 和 NoSQL 数据库 (例如 MongoDB, Redis) 的基本概念和连接方法。
        *   **文件 (CSV, JSON, Excel):**  常见文件格式的特点和 Python 读取方法 (Pandas 读取 CSV, JSON, Excel 文件)。
        *   **非结构化数据 (文本, 图片, 音频, 视频):**  了解非结构化数据的特点和初步处理方法，例如文本数据的分词、图片数据的像素表示。
        *   **动态网页:**  理解动态网页的特点和传统爬虫的局限性。

#### **2. 网络爬虫基础：Requests 库**

*   **知识点:**
    *   **HTTP 协议基础:**  了解 HTTP 请求方法 (GET, POST), 状态码, 请求头, 响应头等基本概念。
    *   **Requests 库:**  安装和导入 Requests 库，掌握 `requests.get()` 和 `requests.post()` 方法的基本用法。
    *   **发送 HTTP 请求:**  使用 Requests 库发送 GET 和 POST 请求，传递 URL 参数和请求体数据。
    *   **处理 HTTP 响应:**  获取响应状态码, 响应头, 响应内容 (文本, JSON, 原始字节流)。
    *   **常用 Requests 功能:**  设置请求头 (User-Agent, Cookies), 超时设置, 代理设置, 会话管理。
    *   **简单的网页抓取案例:**  使用 Requests 库抓取静态网页的 HTML 内容，并初步解析 (例如，使用正则表达式简单提取信息)。

#### **3. 高级爬虫框架：Scrapy 框架**

*   **知识点:**
    *   **Scrapy 框架概述:**  了解 Scrapy 框架的架构 (Spider, Engine, Downloader, Scheduler, Item Pipeline, Middleware) 和优势 (高效, 可扩展, 易于维护)。
    *   **Scrapy 安装与项目创建:**  安装 Scrapy 框架，使用 `scrapy startproject` 创建 Scrapy 项目。
    *   **Spider 的编写:**  定义 Spider 类，编写 `start_urls`, `parse()` 方法，实现网页抓取和数据提取逻辑。
    *   **Selector 的使用:**  学习使用 CSS Selector 和 XPath Selector 从 HTML/XML 文档中提取数据。
    *   **Item 的定义与使用:**  定义 Item 类，用于结构化存储抓取的数据。
    *   **Item Pipeline 的使用:**  编写 Item Pipeline，实现数据清洗、验证、存储等后续处理。
    *   **Scrapy 常用配置:**  设置 `settings.py` 文件，配置 User-Agent, Robots.txt 协议, 并发设置, 下载延迟等。
    *   **中间件 (Middleware) 简介:**  了解 Downloader Middleware 和 Spider Middleware 的作用。
    *   **Scrapy 爬虫案例:**  使用 Scrapy 框架构建更复杂的爬虫，例如抓取商品列表页和详情页信息。

#### **4. 动态网页抓取：Playwright 库**

*   **知识点:**
    *   **动态网页与 JavaScript 渲染:**  理解动态网页的特点，以及 JavaScript 渲染对传统爬虫的影响。
    *   **Playwright 库概述:**  了解 Playwright 库的特点 (支持多种浏览器, 自动化操作, 可靠性高) 和优势。
    *   **Playwright 安装与基本使用:**  安装 Playwright 库，学习启动浏览器 (Chromium, Firefox, WebKit), 打开网页, 获取网页内容的基本操作。
    *   **页面元素操作:**  使用 Playwright 选择器 (CSS Selector, XPath) 定位页面元素，进行点击, 输入, 滚动等操作。
    *   **JavaScript 执行:**  在 Playwright 中执行 JavaScript 代码，获取动态加载的数据。
    *   **等待与超时:**  设置等待条件 (例如，等待元素出现, 等待网络请求完成)，处理页面加载和动态内容加载的延迟。
    *   **处理弹窗和对话框:**  使用 Playwright 处理网页弹窗和对话框。
    *   **Playwright 爬虫案例:**  使用 Playwright 抓取动态网页数据，例如模拟用户登录, 抓取需要 JavaScript 渲染的内容。

#### **5. 数据清洗与转换：Pandas 库**

*   **知识点:**
    *   **Pandas 库概述:**  了解 Pandas 库的核心数据结构 (Series, DataFrame) 和优势 (数据处理, 数据分析)。
    *   **Pandas 安装与数据导入:**  安装 Pandas 库，使用 `pd.read_csv()`, `pd.read_json()`, `pd.read_excel()` 等方法导入数据。
    *   **DataFrame 的基本操作:**  创建 DataFrame, 查看数据 (head, tail, info, describe), 选择列, 选择行 (loc, iloc), 增加列, 删除列。
    *   **数据清洗:**
        *   **处理缺失值:**  识别缺失值 (isnull, notnull), 删除缺失值 (dropna), 填充缺失值 (fillna, 均值/中位数/众数填充, 插值填充)。
        *   **处理重复值:**  识别重复值 (duplicated), 删除重复值 (drop_duplicates)。
        *   **数据类型转换:**  使用 `astype()` 方法转换数据类型 (例如，字符串转数值, 数值转日期)。
        *   **处理异常值:**  识别异常值 (例如，箱线图, Z-score), 处理异常值 (删除, 替换, 忽略)。
        *   **文本数据清洗:**  去除空格, 大小写转换, 字符串替换, 分割字符串。
    *   **数据转换:**
        *   **数据排序:**  使用 `sort_values()` 方法进行排序。
        *   **数据排名:**  使用 `rank()` 方法进行排名。
        *   **数据分组与聚合:**  使用 `groupby()` 方法进行分组，并进行聚合操作 (sum, mean, count, min, max, std)。
        *   **数据透视表:**  使用 `pivot_table()` 和 `crosstab()` 创建透视表和交叉表。
        *   **数据合并与连接:**  使用 `merge()` 和 `concat()` 方法合并和连接 DataFrame。
        *   **数据重塑:**  使用 `stack()` 和 `unstack()` 方法进行数据重塑。

#### **6. 大规模数据处理 (可选)：PySpark 库**

*   **知识点:**
    *   **大数据处理挑战:**  了解传统数据处理方法在大数据场景下的局限性。
    *   **Spark 框架概述:**  了解 Spark 框架的特点 (分布式计算, 内存计算, 高性能) 和优势。
    *   **PySpark 库:**  了解 PySpark 库的作用 (Python 接口, 易用性)。
    *   **SparkContext 和 SparkSession:**  创建 SparkContext 和 SparkSession 对象。
    *   **RDD (弹性分布式数据集):**  了解 RDD 的概念和基本操作 (map, filter, reduce, collect)。
    *   **DataFrame (Spark DataFrame):**  了解 Spark DataFrame 的概念和优势 (结构化数据处理, SQL 支持)。
    *   **Spark DataFrame 基本操作:**  创建 Spark DataFrame, 查看数据, 选择列, 过滤数据, 分组聚合, 连接操作。
    *   **PySpark 应用场景:**  了解 PySpark 在大规模数据清洗、转换、分析中的应用。
    *   **本地模式与集群模式:**  了解 PySpark 的本地模式和集群模式运行方式。
    *   **PySpark 案例:**  使用 PySpark 处理大规模数据集 (例如，日志数据分析, 用户行为数据分析)。

#### **7. 文本数据处理：BeautifulSoup, 正则表达式**

*   **知识点:**
    *   **HTML 和 XML 文档结构:**  了解 HTML 和 XML 文档的基本结构 (标签, 属性, 文本内容)。
    *   **BeautifulSoup 库:**  了解 BeautifulSoup 库的作用 (HTML/XML 解析, 易用性)。
    *   **BeautifulSoup 安装与解析:**  安装 BeautifulSoup 库，使用 `BeautifulSoup()` 函数解析 HTML/XML 文档。
    *   **BeautifulSoup 对象:**  了解 BeautifulSoup 对象 (Tag, NavigableString, BeautifulSoup)。
    *   **BeautifulSoup 查找元素:**  使用 `find()`, `find_all()`, `select()` 等方法查找元素 (根据标签名, 属性, CSS Selector)。
    *   **获取元素属性和文本内容:**  获取元素的属性值和文本内容。
    *   **正则表达式 (Regular Expression):**  了解正则表达式的概念和语法 (元字符, 字符类, 量词, 分组)。
    *   **re 模块:**  Python `re` 模块的基本使用 (compile, search, findall, sub)。
    *   **正则表达式在文本处理中的应用:**  使用正则表达式进行字符串匹配, 查找, 替换, 分割。
    *   **BeautifulSoup 和正则表达式结合应用:**  结合 BeautifulSoup 解析 HTML/XML 文档，并使用正则表达式提取特定模式的文本信息。
    *   **文本数据清洗案例:**  使用 BeautifulSoup 和正则表达式清洗和提取网页文本数据。

### **Python 工具**

*   Requests
*   Scrapy
*   Playwright
*   Pandas
*   PySpark (可选)
*   BeautifulSoup
*   re

### **实践项目**

*   **电商网站商品信息抓取与预处理项目:**
    *   **项目目标:**  使用 Requests, Scrapy, Playwright 抓取不同类型的电商网站 (例如，静态网页, 动态网页) 的商品信息 (例如，商品名称, 价格, 销量, 评价)，并使用 Pandas 清洗和整理数据，最终存储为 CSV 或 JSON 文件。
    *   **项目步骤:**
        1.  **确定目标网站和数据需求:**  选择目标电商网站，明确需要抓取的商品信息字段。
        2.  **分析网页结构:**  分析目标网站的网页结构 (HTML 结构, 动态加载方式)，确定数据提取方法。
        3.  **编写爬虫代码:**
            *   静态网页: 使用 Requests + BeautifulSoup 或 Scrapy 框架。
            *   动态网页: 使用 Playwright。
        4.  **数据提取:**  使用 CSS Selector, XPath, 正则表达式等方法从网页中提取数据。
        5.  **数据清洗与预处理:**  使用 Pandas 进行数据清洗 (缺失值处理, 重复值处理, 数据类型转换, 异常值处理, 文本数据清洗) 和转换。
        6.  **数据存储:**  将清洗后的数据存储为 CSV 或 JSON 文件。
        7.  **项目报告:**  撰写项目报告，总结项目过程, 技术难点, 解决方案, 以及项目成果。

### **模块评估**

*   **平时作业:**  布置与模块主题相关的实践性作业，例如：
    *   使用 Requests 库抓取指定网页的信息。
    *   使用 Scrapy 框架构建简单的爬虫。
    *   使用 Playwright 抓取动态网页数据。
    *   使用 Pandas 进行数据清洗和转换练习。
    *   使用 BeautifulSoup 和正则表达式解析网页文本。
*   **项目评分:**  电商网站商品信息抓取与预处理项目 (小组或个人项目)，根据项目完成度、代码质量、数据质量、项目报告质量进行评分。

### **学习资源**

*   **在线文档:**
    *   Requests 官方文档:  [https://requests.readthedocs.io/en/latest/](https://requests.readthedocs.io/en/latest/)
    *   Scrapy 官方文档:  [https://docs.scrapy.org/en/latest/](https://docs.scrapy.org/en/latest/)
    *   Playwright 官方文档:  [https://playwright.dev/python/docs/intro](https://playwright.dev/python/docs/intro)
    *   Pandas 官方文档:  [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)
    *   PySpark 官方文档:  [https://spark.apache.org/docs/latest/api/python/](https://spark.apache.org/docs/latest/api/python/)
    *   BeautifulSoup 官方文档:  [https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
    *   Python `re` 模块文档:  [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)
*   **参考书籍:**
    *   《Python 网络爬虫开发实战》
    *   《利用 Python 进行数据分析》 (Pandas 作者)
    *   《Python 数据科学手册》

### **总结**

本模块 “数据采集与预处理” 是构建现代商务智能应用的重要基础。 通过本模块的学习，学生将掌握使用 Python 工具进行数据采集和预处理的关键技能，为后续模块的学习和实践项目打下坚实的基础。 掌握这些技能后，学生将能够有效地从各种数据源获取高质量的数据，并为后续的数据分析、建模和智能应用开发做好准备。