"""åŸºç¡€ç½‘ç»œçˆ¬è™«æ¨¡å—ï¼Œä»‹ç»çˆ¬è™«åŸºæœ¬æ¦‚å¿µå’ŒæŠ€æœ¯"""

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup

# å¯¼å…¥æ–‡æœ¬å±•ç¤ºç»„ä»¶
from ...components.text_displays import (
    show_code_block, show_info_card, show_steps, 
    show_tutorial, show_warning_box
)


def show_http_basics():
    """å±•ç¤ºHTTPåŸºç¡€çŸ¥è¯†"""
    st.subheader("HTTPåŸºç¡€çŸ¥è¯†")
    
    st.markdown("""
    HTTP (HyperText Transfer Protocol) æ˜¯ç½‘ç»œé€šä¿¡çš„åŸºç¡€ï¼Œäº†è§£å®ƒå¯¹ç½‘ç»œçˆ¬è™«è‡³å…³é‡è¦ã€‚
    """)
    
    # HTTPè¯·æ±‚æ–¹æ³•
    show_info_card(
        "HTTPè¯·æ±‚æ–¹æ³•", 
        """
        * **GET**: è¯·æ±‚èµ„æºï¼Œå‚æ•°é™„åŠ åœ¨URLä¸­
        * **POST**: æäº¤æ•°æ®ï¼Œå‚æ•°åœ¨è¯·æ±‚ä½“ä¸­
        * **HEAD**: ç±»ä¼¼GETä½†åªè¯·æ±‚å¤´éƒ¨
        * **PUT**: ä¸Šä¼ æˆ–æ›¿æ¢èµ„æº
        * **DELETE**: åˆ é™¤èµ„æº
        
        ç½‘ç»œçˆ¬è™«æœ€å¸¸ç”¨çš„æ˜¯**GET**å’Œ**POST**æ–¹æ³•ã€‚
        """,
        icon="ğŸ”„"
    )
    
    # HTTPçŠ¶æ€ç 
    col1, col2 = st.columns(2)
    
    with col1:
        show_info_card(
            "å¸¸è§HTTPçŠ¶æ€ç ", 
            """
            * **200 OK**: è¯·æ±‚æˆåŠŸ
            * **301/302**: é‡å®šå‘
            * **400**: é”™è¯¯è¯·æ±‚
            * **403**: ç¦æ­¢è®¿é—®
            * **404**: èµ„æºä¸å­˜åœ¨
            * **500**: æœåŠ¡å™¨é”™è¯¯
            """,
            icon="ğŸ”¢"
        )
    
    with col2:
        show_info_card(
            "HTTPå¤´éƒ¨ä¿¡æ¯", 
            """
            * **User-Agent**: å®¢æˆ·ç«¯æ ‡è¯†
            * **Cookie**: ä¼šè¯ä¿¡æ¯
            * **Referer**: æ¥æºé¡µé¢
            * **Content-Type**: å†…å®¹ç±»å‹
            * **Accept**: å¯æ¥å—çš„å“åº”ç±»å‹
            """,
            icon="ğŸ“‹"
        )


def show_requests_tutorial():
    """å±•ç¤ºRequestsåº“æ•™ç¨‹"""
    st.subheader("ä½¿ç”¨Requestså‘é€è¯·æ±‚")
    
    st.markdown("""
    [Requests](https://requests.readthedocs.io/)æ˜¯Pythonä¸­æœ€æµè¡Œçš„HTTPåº“ï¼Œå®ƒè®©HTTPè¯·æ±‚å˜å¾—ç®€å•å’Œäººæ€§åŒ–ã€‚
    """)
    
    # å®‰è£…Requests
    show_code_block(
        "pip install requests",
        language="bash",
        title="å®‰è£…Requests"
    )
    
    # åŸºæœ¬ç”¨æ³•
    show_tutorial(
        "RequestsåŸºæœ¬ç”¨æ³•",
        [
            {
                "title": "å‘é€GETè¯·æ±‚",
                "content": "æœ€ç®€å•çš„GETè¯·æ±‚ç¤ºä¾‹ï¼š",
                "code": """
import requests

# å‘é€GETè¯·æ±‚
response = requests.get('https://httpbin.org/get')

# æŸ¥çœ‹å“åº”çŠ¶æ€ç 
print(response.status_code)  # åº”è¯¥æ˜¯200

# æŸ¥çœ‹å“åº”å†…å®¹
print(response.text)  # è¿”å›æ–‡æœ¬å†…å®¹
print(response.json())  # è§£æJSONå“åº”"""
            },
            {
                "title": "è®¾ç½®è¯·æ±‚å‚æ•°",
                "content": "ä½¿ç”¨paramså‚æ•°å‘é€æŸ¥è¯¢å­—ç¬¦ä¸²ï¼š",
                "code": """
# å¸¦å‚æ•°çš„GETè¯·æ±‚
params = {'key1': 'value1', 'key2': 'value2'}
response = requests.get('https://httpbin.org/get', params=params)

# URLå°†å˜æˆhttps://httpbin.org/get?key1=value1&key2=value2
print(response.url)"""
            },
            {
                "title": "è®¾ç½®è¯·æ±‚å¤´",
                "content": "æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨ä¿¡æ¯ï¼Œæ¯”å¦‚æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼š",
                "code": """
# è®¾ç½®è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
response = requests.get('https://httpbin.org/get', headers=headers)"""
            },
            {
                "title": "å‘é€POSTè¯·æ±‚",
                "content": "å‘é€POSTè¯·æ±‚é€šå¸¸ç”¨äºæäº¤è¡¨å•æ•°æ®ï¼š",
                "code": """
# å‘é€POSTè¯·æ±‚
data = {'username': 'demo', 'password': 'password'}
response = requests.post('https://httpbin.org/post', data=data)

# æŸ¥çœ‹å“åº”
print(response.json())"""
            },
            {
                "title": "å¤„ç†å“åº”",
                "content": "Requestsæä¾›äº†å¤šç§å¤„ç†å“åº”çš„æ–¹æ³•ï¼š",
                "code": """
response = requests.get('https://api.github.com/events')

# æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
if response.status_code == 200:
    # è·å–å“åº”å†…å®¹çš„ä¸åŒå½¢å¼
    text_content = response.text  # æ–‡æœ¬å½¢å¼
    json_content = response.json()  # JSONå½¢å¼ï¼ˆå¦‚æœå“åº”æ˜¯JSONï¼‰
    binary_content = response.content  # äºŒè¿›åˆ¶å½¢å¼

# è·å–å“åº”å¤´ä¿¡æ¯
headers = response.headers
content_type = headers['Content-Type']

# å“åº”ç¼–ç 
print(response.encoding)  # è¿”å›çŒœæµ‹çš„ç¼–ç 
response.encoding = 'utf-8'  # è®¾ç½®ç¼–ç """
            }
        ]
    )
    
    # è¯·æ±‚è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†
    show_code_block(
        """
import requests
from requests.exceptions import RequestException

try:
    # è®¾ç½®5ç§’è¶…æ—¶
    response = requests.get('https://httpbin.org/delay/10', timeout=5)
    response.raise_for_status()  # å¦‚æœçŠ¶æ€ç ä¸æ˜¯200ï¼ŒæŠ›å‡ºå¼‚å¸¸
except requests.Timeout:
    print("è¯·æ±‚è¶…æ—¶")
except requests.HTTPError as e:
    print(f"HTTPé”™è¯¯: {e}")
except RequestException as e:
    print(f"è¯·æ±‚å¼‚å¸¸: {e}")
""",
        title="è¯·æ±‚è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†"
    )
    
    # å®ç”¨æŠ€å·§
    show_warning_box(
        """
        çˆ¬è™«æœ€ä½³å®è·µï¼š
        1. æ€»æ˜¯æ·»åŠ è¶…æ—¶è®¾ç½®ï¼Œé¿å…ç¨‹åºæ— é™ç­‰å¾…
        2. å§‹ç»ˆè¿›è¡Œå¼‚å¸¸å¤„ç†
        3. å°Šé‡æœåŠ¡å™¨ï¼Œæ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼ˆè€ƒè™‘ä½¿ç”¨time.sleep()ï¼‰
        4. ä½¿ç”¨User-Agentæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé¿å…è¢«æ‹’ç»è®¿é—®
        """,
        title="çˆ¬è™«å®ç”¨æŠ€å·§"
    )


def show_beautifulsoup_tutorial():
    """å±•ç¤ºBeautifulSoupæ•™ç¨‹"""
    st.subheader("ä½¿ç”¨BeautifulSoupè§£æHTML")
    
    st.markdown("""
    [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)æ˜¯ä¸€ä¸ªç”¨äºä»HTMLå’ŒXMLæ–‡ä»¶ä¸­æå–æ•°æ®çš„Pythonåº“ã€‚
    å®ƒæä¾›äº†ç®€å•çš„æ–¹æ³•æ¥å¯¼èˆªã€æœç´¢å’Œä¿®æ”¹è§£ææ ‘ï¼Œéå¸¸é€‚åˆç½‘é¡µæ•°æ®æå–ã€‚
    """)
    
    # å®‰è£…BeautifulSoup
    show_code_block(
        "pip install beautifulsoup4 lxml",
        language="bash",
        title="å®‰è£…BeautifulSoupå’Œè§£æå™¨"
    )
    
    # åŸºæœ¬ç”¨æ³•
    show_tutorial(
        "BeautifulSoupåŸºæœ¬ç”¨æ³•",
        [
            {
                "title": "åˆ›å»ºBeautifulSoupå¯¹è±¡",
                "content": "é¦–å…ˆéœ€è¦å°†HTMLæ–‡æ¡£è§£æä¸ºBeautifulSoupå¯¹è±¡ï¼š",
                "code": """
from bs4 import BeautifulSoup

# HTMLå­—ç¬¦ä¸²
html_doc = '''
<html>
    <head><title>ç½‘é¡µæ ‡é¢˜</title></head>
    <body>
        <h1 id="main-title">Hello World</h1>
        <p class="content">è¿™æ˜¯ä¸€ä¸ªæ®µè½ã€‚</p>
        <p class="content">è¿™æ˜¯å¦ä¸€ä¸ªæ®µè½ã€‚</p>
        <ul>
            <li><a href="https://example.com/page1">é“¾æ¥1</a></li>
            <li><a href="https://example.com/page2">é“¾æ¥2</a></li>
        </ul>
    </body>
</html>
'''

# åˆ›å»ºBeautifulSoupå¯¹è±¡
soup = BeautifulSoup(html_doc, 'lxml')  # ä½¿ç”¨lxmlè§£æå™¨

# æ ¼å¼åŒ–è¾“å‡ºHTML
print(soup.prettify())"""
            },
            {
                "title": "ç®€å•å¯¼èˆªå’Œæœç´¢",
                "content": "BeautifulSoupæä¾›äº†å¤šç§æ–¹æ³•æ¥å¯¼èˆªå’Œæœç´¢è§£ææ ‘ï¼š",
                "code": """
# æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç­¾
title_tag = soup.title
print(title_tag)  # <title>ç½‘é¡µæ ‡é¢˜</title>
print(title_tag.string)  # ç½‘é¡µæ ‡é¢˜

# æŸ¥æ‰¾æ‰€æœ‰æ ‡ç­¾
all_paragraphs = soup.find_all('p')
for p in all_paragraphs:
    print(p.text)

# é€šè¿‡å±æ€§æŸ¥æ‰¾
main_title = soup.find(id='main-title')
print(main_title.text)  # Hello World

# é€šè¿‡CSSé€‰æ‹©å™¨æŸ¥æ‰¾
content_paragraphs = soup.select('.content')
for p in content_paragraphs:
    print(p.text)"""
            },
            {
                "title": "CSSé€‰æ‹©å™¨",
                "content": "BeautifulSoupæ”¯æŒå¤šç§CSSé€‰æ‹©å™¨è¯­æ³•ï¼š",
                "code": """
# é€šè¿‡æ ‡ç­¾åæŸ¥æ‰¾
all_links = soup.select('a')

# é€šè¿‡ç±»åæŸ¥æ‰¾
content_elements = soup.select('.content')

# é€šè¿‡IDæŸ¥æ‰¾
main_title = soup.select('#main-title')

# ç»„åˆæŸ¥æ‰¾
list_links = soup.select('ul li a')

# æŸ¥æ‰¾å±æ€§
links = soup.select('a[href^="https"]')  # ä»¥httpså¼€å¤´çš„é“¾æ¥"""
            },
            {
                "title": "æå–æ•°æ®",
                "content": "ä»æ ‡ç­¾ä¸­æå–æ•°æ®ï¼š",
                "code": """
# æå–æ–‡æœ¬å†…å®¹
for paragraph in soup.find_all('p'):
    print(paragraph.text)  # è·å–æ–‡æœ¬å†…å®¹

# æå–å±æ€§å€¼
for link in soup.find_all('a'):
    print(link.get('href'))  # è·å–hrefå±æ€§
    # æˆ–è€…ä½¿ç”¨ link['href']"""
            }
        ]
    )
    
    # å®é™…ç¤ºä¾‹
    show_code_block(
        """
import requests
from bs4 import BeautifulSoup

# è·å–ç½‘é¡µå†…å®¹
url = 'https://news.ycombinator.com/'
response = requests.get(url)
html_content = response.text

# è§£æHTML
soup = BeautifulSoup(html_content, 'lxml')

# æå–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥
news_items = []
for item in soup.select('.titleline > a'):
    title = item.text
    link = item.get('href')
    news_items.append({'title': title, 'link': link})

# æ˜¾ç¤ºç»“æœ
for item in news_items[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
    print(f"æ ‡é¢˜: {item['title']}")
    print(f"é“¾æ¥: {item['link']}")
    print('-' * 50)
""",
        title="å®é™…çˆ¬è™«ç¤ºä¾‹ï¼šçˆ¬å–Hacker Newså¤´æ¡"
    )
    
    # æç¤ºå’ŒæŠ€å·§
    show_info_card(
        "BeautifulSoupæŠ€å·§", 
        """
        * ä½¿ç”¨**.find()**æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…å…ƒç´ ï¼Œä½¿ç”¨**.find_all()**æ‰¾æ‰€æœ‰åŒ¹é…å…ƒç´ 
        * **.select()**æ–¹æ³•æ”¯æŒCSSé€‰æ‹©å™¨ï¼ŒåŠŸèƒ½å¼ºå¤§ä¸”ç›´è§‚
        * ç»“åˆ**æ­£åˆ™è¡¨è¾¾å¼**å¯ä»¥å®ç°æ›´å¤æ‚çš„åŒ¹é…
        * å¯¹äºå¤„ç†å¤§å‹æ–‡æ¡£ï¼Œè€ƒè™‘ä½¿ç”¨**lxml**è§£æå™¨æé«˜æ€§èƒ½
        * **NavigableString**å¯¹è±¡è¡¨ç¤ºæ ‡ç­¾å†…çš„æ–‡æœ¬å†…å®¹
        * ä½¿ç”¨**.decompose()**æ–¹æ³•å¯ä»¥ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ ï¼Œå¦‚å¹¿å‘Š
        """,
        icon="ğŸ’¡"
    )


def show_data_extraction():
    """å±•ç¤ºæ•°æ®æå–æŠ€æœ¯"""
    st.subheader("æå–ç½‘é¡µæ•°æ®")
    
    st.markdown("""
    ç½‘é¡µæ•°æ®æå–æ˜¯çˆ¬è™«çš„æ ¸å¿ƒä»»åŠ¡ã€‚å€ŸåŠ©Requestså’ŒBeautifulSoupçš„ç»„åˆï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°ä»ç½‘é¡µä¸­æå–æ‰€éœ€ä¿¡æ¯ã€‚
    """)
    
    # å¸¸è§æ•°æ®æå–æ¨¡å¼
    show_steps(
        [
            {
                "title": "è¯†åˆ«ç›®æ ‡æ•°æ®",
                "content": """
                åœ¨å¼€å§‹çˆ¬å–ä¹‹å‰ï¼Œå…ˆæ˜ç¡®éœ€è¦è·å–çš„æ•°æ®ç±»å‹ï¼š
                * æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ã€æ®µè½ã€æè¿°ç­‰ï¼‰
                * é“¾æ¥URL
                * å›¾ç‰‡URL
                * è¡¨æ ¼æ•°æ®
                * åˆ—è¡¨æ•°æ®
                
                ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ï¼ˆF12ï¼‰æ£€æŸ¥ç›®æ ‡å…ƒç´ çš„HTMLç»“æ„ï¼Œæ‰¾å‡ºå”¯ä¸€æ ‡è¯†å®ƒä»¬çš„ç‰¹å¾ï¼ˆIDã€ç±»åã€å±æ€§ç­‰ï¼‰ã€‚
                """
            },
            {
                "title": "è·å–ç½‘é¡µå†…å®¹",
                "content": "ä½¿ç”¨Requestsåº“è·å–ç½‘é¡µHTMLå†…å®¹",
                "code": """
import requests

url = "https://example.com"
response = requests.get(url)
html_content = response.text"""
            },
            {
                "title": "è§£æé¡µé¢ç»“æ„",
                "content": "ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œåˆ›å»ºå¯æŸ¥è¯¢çš„è§£ææ ‘",
                "code": """
from bs4 import BeautifulSoup

soup = BeautifulSoup(html_content, 'lxml')"""
            },
            {
                "title": "å®šä½å’Œæå–æ•°æ®",
                "content": "ä½¿ç”¨BeautifulSoupçš„æŸ¥è¯¢æ–¹æ³•å®šä½å¹¶æå–ç›®æ ‡æ•°æ®",
                "code": """
# æå–æ‰€æœ‰æ ‡é¢˜
titles = []
for heading in soup.find_all(['h1', 'h2', 'h3']):
    titles.append(heading.text.strip())

# æå–æ‰€æœ‰é“¾æ¥
links = []
for a_tag in soup.find_all('a'):
    link = a_tag.get('href')
    if link and not link.startswith('#'):  # è¿‡æ»¤é¡µå†…é”šç‚¹
        links.append(link)
        
# æå–è¡¨æ ¼æ•°æ®
tables_data = []
for table in soup.find_all('table'):
    table_data = []
    for row in table.find_all('tr'):
        row_data = [cell.text.strip() for cell in row.find_all(['td', 'th'])]
        if row_data:  # ç¡®ä¿è¡Œä¸ä¸ºç©º
            table_data.append(row_data)
    tables_data.append(table_data)"""
            },
            {
                "title": "æ•°æ®æ¸…ç†ä¸ç»“æ„åŒ–",
                "content": "æ¸…ç†æå–çš„æ•°æ®å¹¶ç»„ç»‡æˆç»“æ„åŒ–æ ¼å¼",
                "code": """
import pandas as pd

# æ¸…ç†æ–‡æœ¬æ•°æ®
def clean_text(text):
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = ' '.join(text.split())
    # å…¶ä»–æ¸…ç†æ“ä½œ...
    return text

# å°†æå–çš„æ•°æ®è½¬æ¢ä¸ºDataFrame
if tables_data:
    # å‡è®¾ç¬¬ä¸€ä¸ªè¡¨æ ¼çš„ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
    headers = tables_data[0][0]
    data = tables_data[0][1:]
    df = pd.DataFrame(data, columns=headers)
    
    # æ•°æ®æ¸…æ´—
    df = df.applymap(clean_text)
    
    print(df.head())"""
            },
            {
                "title": "å­˜å‚¨æ•°æ®",
                "content": "å°†æå–çš„æ•°æ®ä¿å­˜ä¸ºéœ€è¦çš„æ ¼å¼",
                "code": """
# ä¿å­˜ä¸ºCSV
df.to_csv('extracted_data.csv', index=False)

# ä¿å­˜ä¸ºJSON
df.to_json('extracted_data.json', orient='records')

# ä¿å­˜ä¸ºExcel
df.to_excel('extracted_data.xlsx', index=False)"""
            }
        ],
        title="æ•°æ®æå–æ­¥éª¤"
    )
    
    # æå–ä¸åŒç±»å‹çš„æ•°æ®ç¤ºä¾‹
    example_tabs = st.tabs(["æå–æ–‡æœ¬", "æå–é“¾æ¥", "æå–è¡¨æ ¼", "æå–å›¾ç‰‡"])
    
    with example_tabs[0]:
        show_code_block(
            """
# æå–æ–‡ç« æ­£æ–‡
article_content = soup.find('article', class_='post-content')
if article_content:
    # æå–æ‰€æœ‰æ®µè½æ–‡æœ¬
    paragraphs = [p.text.strip() for p in article_content.find_all('p')]
    
    # åˆå¹¶æ®µè½
    full_text = '\\n\\n'.join(paragraphs)
    
    # ç§»é™¤å¤šä½™ç©ºç™½
    import re
    full_text = re.sub(r'\\s+', ' ', full_text)
            """,
            title="æå–æ–‡ç« æ­£æ–‡"
        )
    
    with example_tabs[1]:
        show_code_block(
            """
# æå–æ‰€æœ‰é“¾æ¥å¹¶è§„èŒƒåŒ–URL
from urllib.parse import urljoin

base_url = "https://example.com"
links = []

for a_tag in soup.find_all('a', href=True):
    href = a_tag.get('href')
    
    # å°†ç›¸å¯¹URLè½¬ä¸ºç»å¯¹URL
    absolute_url = urljoin(base_url, href)
    
    # æå–é“¾æ¥æ–‡æœ¬
    link_text = a_tag.text.strip()
    
    links.append({
        'text': link_text,
        'url': absolute_url
    })
            """,
            title="æå–å’Œè§„èŒƒåŒ–é“¾æ¥"
        )
    
    with example_tabs[2]:
        show_code_block(
            """
# æå–è¡¨æ ¼æ•°æ®åˆ°Pandas DataFrame
import pandas as pd

# å®šä½è¡¨æ ¼
table = soup.find('table', id='data-table')  # æˆ–è€…ä½¿ç”¨class_='table-class'
if table:
    # æå–è¡¨å¤´
    headers = []
    header_row = table.find('thead').find('tr')
    for th in header_row.find_all('th'):
        headers.append(th.text.strip())
    
    # æå–è¡¨æ ¼æ•°æ®
    rows = []
    for tr in table.find('tbody').find_all('tr'):
        row = []
        for td in tr.find_all('td'):
            row.append(td.text.strip())
        rows.append(row)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(rows, columns=headers)
    print(df.head())
            """,
            title="æå–è¡¨æ ¼æ•°æ®åˆ°DataFrame"
        )
    
    with example_tabs[3]:
        show_code_block(
            """
# æå–æ‰€æœ‰å›¾ç‰‡URL
import os
from urllib.parse import urljoin

base_url = "https://example.com"
image_info = []

for img in soup.find_all('img', src=True):
    # è·å–å›¾ç‰‡URL
    img_url = img.get('src')
    absolute_img_url = urljoin(base_url, img_url)
    
    # è·å–altæ–‡æœ¬
    alt_text = img.get('alt', '')
    
    # è·å–å›¾ç‰‡æ–‡ä»¶å
    file_name = os.path.basename(img_url)
    
    image_info.append({
        'url': absolute_img_url,
        'alt': alt_text,
        'file_name': file_name
    })

# ä¸‹è½½å›¾ç‰‡
def download_image(img_url, save_path):
    response = requests.get(img_url, stream=True)
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        return True
    return False

# ä¸‹è½½å‰5å¼ å›¾ç‰‡ç¤ºä¾‹
for i, img in enumerate(image_info[:5]):
    save_path = f"image_{i}_{img['file_name']}"
    success = download_image(img['url'], save_path)
    print(f"ä¸‹è½½ {img['url']} åˆ° {save_path}: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            """,
            title="æå–å’Œä¸‹è½½å›¾ç‰‡"
        )
    
    # æä¾›ä¸€ä¸ªç»¼åˆæ¡ˆä¾‹
    st.markdown("### ç»¼åˆæ¡ˆä¾‹ï¼šæå–ç½‘ç«™äº§å“ä¿¡æ¯")
    
    show_code_block(
        """
import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin
import time
import random

def scrape_product_info(url):
    # æ·»åŠ è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5'
    }
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # æ£€æŸ¥å“åº”çŠ¶æ€
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'lxml')
        
        # äº§å“åˆ—è¡¨å®¹å™¨ (å‡è®¾äº§å“åœ¨å…·æœ‰ç‰¹å®šç±»çš„divä¸­)
        products = soup.find_all('div', class_='product-item')
        
        all_products = []
        
        for product in products:
            # æå–äº§å“ä¿¡æ¯
            title_element = product.find('h2', class_='product-title')
            price_element = product.find('span', class_='price')
            rating_element = product.find('div', class_='rating')
            image_element = product.find('img', class_='product-image')
            link_element = product.find('a', class_='product-link')
            
            # æå–æ•°æ®ï¼Œæä¾›é»˜è®¤å€¼é˜²æ­¢Noneé”™è¯¯
            title = title_element.text.strip() if title_element else 'No Title'
            price = price_element.text.strip() if price_element else 'No Price'
            rating = rating_element.get('data-rating', 'No Rating') if rating_element else 'No Rating'
            image_url = image_element.get('src') if image_element else None
            product_url = link_element.get('href') if link_element else None
            
            # è§„èŒƒåŒ–URL
            if image_url:
                image_url = urljoin(url, image_url)
            if product_url:
                product_url = urljoin(url, product_url)
            
            # æ•´åˆæ•°æ®
            product_info = {
                'title': title,
                'price': price,
                'rating': rating,
                'image_url': image_url,
                'product_url': product_url
            }
            
            all_products.append(product_info)
            
            # å‹å¥½çˆ¬å–ï¼Œéšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(0.5, 2.0))
        
        # è½¬æ¢ä¸ºDataFrame
        products_df = pd.DataFrame(all_products)
        
        return products_df
    
    except Exception as e:
        print(f"çˆ¬å–å‡ºé”™: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    target_url = "https://example.com/products"
    df = scrape_product_info(target_url)
    
    if df is not None:
        print(f"æˆåŠŸçˆ¬å– {len(df)} ä¸ªäº§å“ä¿¡æ¯")
        df.to_csv('product_data.csv', index=False)
        print("æ•°æ®å·²ä¿å­˜åˆ° product_data.csv")
        """,
        title="ç»¼åˆæ•°æ®æå–æ¡ˆä¾‹"
    )


def show_simple_example():
    """å±•ç¤ºç®€å•çˆ¬è™«å®ä¾‹"""
    st.subheader("ç®€å•çˆ¬è™«å®ä¾‹")
    
    st.markdown("""
    ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•ä½†å®Œæ•´çš„çˆ¬è™«ç¤ºä¾‹ï¼Œå®ƒçˆ¬å–ä¸€ä¸ªç½‘é¡µçš„æ ‡é¢˜å’Œæ‰€æœ‰é“¾æ¥ï¼š
    """)
    
    # ä»£ç ç¤ºä¾‹
    show_code_block(
        """
import requests
from bs4 import BeautifulSoup
import pandas as pd
import logging
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("scraper.log"),
        logging.StreamHandler()
    ]
)

def scrape_website(url, max_retries=3):
    """
    çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µæ ‡é¢˜å’Œé“¾æ¥
    
    å‚æ•°:
        url (str): è¦çˆ¬å–çš„ç½‘é¡µURL
        max_retries (int): æœ€å¤§é‡è¯•æ¬¡æ•°
    
    è¿”å›:
        dict: åŒ…å«æ ‡é¢˜å’Œé“¾æ¥åˆ—è¡¨çš„å­—å…¸
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    retry_count = 0
    while retry_count < max_retries:
        try:
            logging.info(f"æ­£åœ¨è¯·æ±‚URL: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # è·å–ç½‘é¡µæ ‡é¢˜å’Œé“¾æ¥
            soup = BeautifulSoup(response.text, 'lxml')
            title = soup.title.text if soup.title else "æ— æ ‡é¢˜"
            
            links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.text.strip()
                if href and not href.startswith('#'):
                    links.append({
                        'url': href,
                        'text': text if text else "æ— æ–‡æœ¬"
                    })
            
            logging.info(f"æˆåŠŸæŠ“å– {url}, è·å–åˆ° {len(links)} ä¸ªé“¾æ¥")
            
            return {
                'title': title,
                'links': links
            }
            
        except requests.exceptions.RequestException as e:
            retry_count += 1
            wait_time = 2 ** retry_count  # æŒ‡æ•°é€€é¿
            logging.error(f"è¯·æ±‚å¤±è´¥ ({retry_count}/{max_retries}): {e}")
            logging.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
            time.sleep(wait_time)
    
    logging.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè¯·æ±‚: {url}")
    return None

def save_data(data, output_file='scrape_results.csv'):
    """ä¿å­˜çˆ¬å–çš„é“¾æ¥æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if not data or 'links' not in data:
        logging.error("æ— æ•°æ®å¯ä¿å­˜")
        return False
    
    try:
        df = pd.DataFrame(data['links'])
        df.to_csv(output_file, index=False)
        logging.info(f"æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    url = "https://example.com"  # æ›¿æ¢ä¸ºä½ è¦çˆ¬å–çš„URL
    
    # çˆ¬å–æ•°æ®
    data = scrape_website(url)
    
    if data:
        # æ‰“å°ç½‘é¡µæ ‡é¢˜
        print(f"ç½‘é¡µæ ‡é¢˜: {data['title']}")
        
        # ä¿å­˜é“¾æ¥åˆ°CSV
        save_data(data)
        
        # æ‰“å°å‰5ä¸ªé“¾æ¥
        print("\nå‰5ä¸ªé“¾æ¥:")
        for i, link in enumerate(data['links'][:5], 1):
            print(f"{i}. {link['text']} - {link['url']}")
    else:
        print("çˆ¬å–å¤±è´¥")

if __name__ == "__main__":
    main()
        """,
        title="å®Œæ•´çˆ¬è™«ç¤ºä¾‹"
    )
    
    # çˆ¬è™«é¡¹ç›®ç»“æ„
    st.markdown("### çˆ¬è™«é¡¹ç›®ç»“æ„")
    
    project_structure = """
scraper/
â”œâ”€â”€ main.py           # ä¸»å…¥å£æ–‡ä»¶
â”œâ”€â”€ scraper.py        # çˆ¬è™«æ ¸å¿ƒåŠŸèƒ½
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ parser.py     # HTMLè§£æåŠŸèƒ½
â”‚   â”œâ”€â”€ downloader.py # ä¸‹è½½å™¨
â”‚   â””â”€â”€ logger.py     # æ—¥å¿—é…ç½®
â”œâ”€â”€ config.py         # é…ç½®å‚æ•°
â”œâ”€â”€ requirements.txt  # ä¾èµ–é¡¹
â””â”€â”€ data/             # çˆ¬å–æ•°æ®ä¿å­˜ç›®å½•
    â””â”€â”€ .gitkeep
"""
    
    st.code(project_structure)
    
    # å®ç”¨æŠ€å·§
    show_info_card(
        "çˆ¬è™«å®ç”¨æŠ€å·§", 
        """
        1. **å¢é‡å¼çˆ¬å–**ï¼šåªçˆ¬å–æ–°å†…å®¹ï¼Œå‡å°‘æœåŠ¡å™¨è´Ÿæ‹…
        2. **ä½¿ç”¨ä»£ç†IP**ï¼šé¿å…IPè¢«å°
        3. **éšæœºå»¶è¿Ÿ**ï¼šæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œå‡å°‘è¢«æ£€æµ‹é£é™©
        4. **ç¼–å†™æµ‹è¯•**ï¼šç¡®ä¿çˆ¬è™«åœ¨ç½‘ç«™ç»“æ„å˜åŒ–æ—¶èƒ½å¤Ÿå¿«é€Ÿé€‚åº”
        5. **å®šæœŸç»´æŠ¤**ï¼šç½‘ç«™ç»“æ„å¯èƒ½éšæ—¶å˜åŒ–ï¼Œéœ€è¦ç»å¸¸æ›´æ–°çˆ¬è™«
        6. **ä¿å­˜åŸå§‹æ•°æ®**ï¼šä¿å­˜åŸå§‹HTMLæœ‰åŠ©äºæ—¥åé‡æ–°è§£æ
        """,
        icon="ğŸš€"
    )


def show_basics():
    """å±•ç¤ºåŸºç¡€ç½‘ç»œçˆ¬è™«æ•™ç¨‹"""
    st.title("ç½‘ç»œçˆ¬è™«åŸºç¡€")
    
    # ä»‹ç»
    st.markdown("""
    ç½‘ç»œçˆ¬è™«ï¼ˆWeb Crawleræˆ–Web Spiderï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–ç¨‹åºï¼Œç”¨äºç³»ç»Ÿåœ°æµè§ˆä¸‡ç»´ç½‘å¹¶æ”¶é›†æ•°æ®ã€‚
    åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ åŸºç¡€çš„ç½‘ç»œçˆ¬è™«æŠ€æœ¯ï¼ŒåŒ…æ‹¬HTTPåŸºç¡€çŸ¥è¯†ã€ä½¿ç”¨Requestså‘é€è¯·æ±‚ã€
    ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œä»¥åŠå¦‚ä½•æå–å’Œå­˜å‚¨æ•°æ®ã€‚
    """)
    
    # ç›®å½•
    toc = st.selectbox(
        "é€‰æ‹©å­¦ä¹ å†…å®¹",
        ["HTTPåŸºç¡€çŸ¥è¯†", "ä½¿ç”¨Requestså‘é€è¯·æ±‚", "ä½¿ç”¨BeautifulSoupè§£æHTML", "æå–ç½‘é¡µæ•°æ®", "ç®€å•çˆ¬è™«å®ä¾‹"]
    )
    
    st.markdown("---")
    
    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºç›¸åº”å†…å®¹
    if toc == "HTTPåŸºç¡€çŸ¥è¯†":
        show_http_basics()
    elif toc == "ä½¿ç”¨Requestså‘é€è¯·æ±‚":
        show_requests_tutorial()
    elif toc == "ä½¿ç”¨BeautifulSoupè§£æHTML":
        show_beautifulsoup_tutorial()
    elif toc == "æå–ç½‘é¡µæ•°æ®":
        show_data_extraction()
    elif toc == "ç®€å•çˆ¬è™«å®ä¾‹":
        show_simple_example()
    
    # äº’åŠ¨ç»„ä»¶ - ç®€å•çˆ¬è™«æµ‹è¯•
    st.markdown("---")
    st.subheader("äº’åŠ¨æµ‹è¯•ï¼šé“¾æ¥æå–å™¨")
    
    with st.expander("å°è¯•ä¸€ä¸‹é“¾æ¥æå–å™¨"):
        test_url = st.text_input("è¾“å…¥ä¸€ä¸ªç½‘å€", "https://www.example.com")
        
        if st.button("æå–é“¾æ¥"):
            try:
                with st.spinner("æ­£åœ¨è·å–æ•°æ®..."):
                    # å‘é€è¯·æ±‚
                    response = requests.get(test_url, timeout=10)
                    
                    # è§£æHTML
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æå–æ‰€æœ‰é“¾æ¥
                    links = []
                    for a_tag in soup.find_all('a', href=True):
                        href = a_tag.get('href')
                        text = a_tag.text.strip()
                        links.append({"text": text if text else "æ— æ–‡æœ¬", "url": href})
                    
                    # æ˜¾ç¤ºç»“æœ
                    if links:
                        st.success(f"æˆåŠŸæå– {len(links)} ä¸ªé“¾æ¥")
                        df = pd.DataFrame(links)
                        st.dataframe(df)
                    else:
                        st.info("æ²¡æœ‰æ‰¾åˆ°é“¾æ¥")
                        
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")


if __name__ == "__main__":
    show_basics() 