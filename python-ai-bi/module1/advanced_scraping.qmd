# 高级爬虫框架

在上一章节中，我们学习了使用Requests库进行基础网络爬虫开发。虽然Requests库足以应对简单的爬虫任务，但对于大规模、分布式的爬虫项目，我们需要更加强大和结构化的工具。Scrapy是目前Python生态中最流行的爬虫框架，本章将详细介绍其使用方法。

## Scrapy简介

Scrapy是一个用Python编写的开源爬虫框架，专为大规模爬取网站数据而设计。它提供了一套完整的爬虫解决方案，包括数据提取、数据处理和数据存储等功能。

:::{.important}
Scrapy的主要特点包括：

1. **高效性**：异步网络库支持并发请求，大幅提高爬取效率
2. **可扩展性**：模块化设计，易于扩展和定制
3. **内置中间件**：提供请求/响应处理、Spider中间件等
4. **强大的选择器**：支持XPath和CSS选择器
5. **数据导出**：支持多种格式（JSON、CSV、XML等）
6. **分布式支持**：可与Scrapy-Redis等工具结合实现分布式爬取
:::

## Scrapy架构

Scrapy采用了组件化的架构设计，各组件之间通过事件和消息传递进行通信。

![Scrapy架构图](../images/scrapy_architecture.png)

Scrapy的主要组件包括：

1. **Engine（引擎）**：负责控制数据流在系统中的处理流程，以及触发事件
2. **Scheduler（调度器）**：接收引擎发来的请求，按照一定的方式进行整理排列，入队，当引擎需要时提供请求
3. **Downloader（下载器）**：负责获取页面数据并返回给引擎
4. **Spider（爬虫）**：负责处理网页并提取结构化数据
5. **Item Pipeline（项目管道）**：负责处理Spider提取的数据
6. **Downloader Middleware（下载中间件）**：位于引擎和下载器之间，处理下载器传递给引擎的响应
7. **Spider Middleware（爬虫中间件）**：位于引擎和爬虫之间，处理爬虫的输入（响应）和输出（项目和请求）

## 安装Scrapy

```python
pip install scrapy
```

## 创建Scrapy项目

使用命令行创建一个新的Scrapy项目：

```bash
scrapy startproject tutorial
```

这将创建以下文件结构：

```
tutorial/
    scrapy.cfg          # 项目配置文件
    tutorial/           # 项目Python模块
        __init__.py
        items.py        # 项目项定义文件
        middlewares.py  # 中间件定义文件
        pipelines.py    # 项目管道定义文件
        settings.py     # 项目设置文件
        spiders/        # 放置spider的目录
            __init__.py
```

## 定义Item

在items.py文件中定义要抓取的数据结构：

```python
# tutorial/items.py
import scrapy

class TutorialItem(scrapy.Item):
    # 定义需要抓取的字段
    title = scrapy.Field()
    author = scrapy.Field()
    content = scrapy.Field()
    tags = scrapy.Field()
    date = scrapy.Field()
```

## 创建Spider

在spiders目录下创建一个新的Spider：

```python
# tutorial/spiders/quotes_spider.py
import scrapy
from tutorial.items import TutorialItem

class QuotesSpider(scrapy.Spider):
    # Spider的唯一标识
    name = "quotes"
    
    # 允许爬取的域名
    allowed_domains = ["quotes.toscrape.com"]
    
    # 起始URL列表
    start_urls = [
        'http://quotes.toscrape.com/',
    ]
    
    # 处理响应的方法
    def parse(self, response):
        # 提取所有引用
        for quote in response.css('div.quote'):
            item = TutorialItem()
            item['title'] = quote.css('span.text::text').get()
            item['author'] = quote.css('small.author::text').get()
            item['tags'] = quote.css('div.tags a.tag::text').getall()
            yield item
        
        # 提取下一页链接并跟随
        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

## 运行Spider

在项目根目录下运行Spider：

```bash
scrapy crawl quotes
```

## 数据存储

Scrapy提供了多种数据存储方式，最简单的是使用Feed exports功能：

```bash
scrapy crawl quotes -o quotes.json
```

这将把抓取的数据保存为JSON格式。

### 自定义Item Pipeline

如果需要更复杂的数据处理，可以使用Item Pipeline：

```python
# tutorial/pipelines.py
import json

class JsonWriterPipeline:
    def open_spider(self, spider):
        self.file = open('items.jl', 'w')
    
    def close_spider(self, spider):
        self.file.close()
    
    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item
```

然后在settings.py中启用Pipeline：

```python
# tutorial/settings.py
ITEM_PIPELINES = {
    'tutorial.pipelines.JsonWriterPipeline': 300,
}
```

## 中间件使用

Scrapy的中间件可以处理请求和响应，常用于：

1. 设置自定义请求头
2. 处理cookies
3. 实现代理IP池
4. 处理重定向
5. 实现请求重试机制

### 自定义下载中间件

```python
# tutorial/middlewares.py
import random
from scrapy import signals

class RandomUserAgentMiddleware:
    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
        ]
    
    def process_request(self, request, spider):
        request.headers['User-Agent'] = random.choice(self.user_agents)
        return None
```

在settings.py中启用中间件：

```python
# tutorial/settings.py
DOWNLOADER_MIDDLEWARES = {
    'tutorial.middlewares.RandomUserAgentMiddleware': 543,
}
```

## 爬虫配置与优化

Scrapy提供了丰富的配置选项，以下是一些常用的配置：

### 并发请求数控制

```python
# 默认同时处理的请求数量
CONCURRENT_REQUESTS = 16

# 对单个域名的并发请求数
CONCURRENT_REQUESTS_PER_DOMAIN = 8

# 对同一IP的并发请求数
CONCURRENT_REQUESTS_PER_IP = 0
```

### 请求延迟

```python
# 下载延迟（秒）
DOWNLOAD_DELAY = 1

# 是否启用自动限速
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 5
AUTOTHROTTLE_MAX_DELAY = 60
```

### 断点续爬

Scrapy支持使用Jobs功能实现断点续爬：

```bash
# 开始爬取并保存状态
scrapy crawl quotes -s JOBDIR=crawls/quotes-1

# 中断后继续爬取
scrapy crawl quotes -s JOBDIR=crawls/quotes-1
```

## 实践任务：开发一个电商网站爬虫

:::{.task}
使用Scrapy开发一个爬虫，抓取电商网站的商品信息：

1. 创建Scrapy项目
2. 定义商品项（名称、价格、评分、评论数等）
3. 编写爬虫逻辑
4. 处理分页
5. 存储数据到CSV文件

以下是一个简单的示例：

```python
# 创建项目
# scrapy startproject ecommerce_scraper

# 定义Item
# ecommerce_scraper/items.py
import scrapy

class ProductItem(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    rating = scrapy.Field()
    reviews_count = scrapy.Field()
    url = scrapy.Field()

# 编写爬虫
# ecommerce_scraper/spiders/product_spider.py
import scrapy
from ecommerce_scraper.items import ProductItem

class ProductSpider(scrapy.Spider):
    name = "products"
    allowed_domains = ["example.com"]
    start_urls = ["https://example.com/products"]
    
    def parse(self, response):
        # 提取产品信息
        for product in response.css('div.product'):
            item = ProductItem()
            item['name'] = product.css('h2.title::text').get()
            item['price'] = product.css('span.price::text').get()
            item['rating'] = product.css('div.rating::attr(data-rating)').get()
            item['reviews_count'] = product.css('span.reviews-count::text').get()
            item['url'] = product.css('a.product-link::attr(href)').get()
            yield item
        
        # 处理分页
        next_page = response.css('a.next-page::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)

# 运行爬虫并保存为CSV
# scrapy crawl products -o products.csv
```
:::

## Scrapy的高级功能

### 1. Scrapy Shell

Scrapy Shell是一个交互式环境，用于测试提取数据的代码：

```bash
scrapy shell "http://quotes.toscrape.com/"
```

在Shell中，你可以使用`response`对象测试选择器：

```python
response.css('div.quote')
```

### 2. 使用CrawlSpider自动发现链接

CrawlSpider是Scrapy内置的一种Spider类型，可以根据定义的规则自动发现和跟踪链接：

```python
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'crawler'
    allowed_domains = ['example.com']
    start_urls = ['https://example.com']
    
    rules = (
        # 提取匹配 'category' 的链接并跟进
        Rule(LinkExtractor(allow=r'category'), follow=True),
        # 提取匹配 'item' 的链接并使用parse_item解析
        Rule(LinkExtractor(allow=r'item'), callback='parse_item'),
    )
    
    def parse_item(self, response):
        # 处理项页面
        # ...
        pass
```

### 3. 集成Selenium处理JavaScript

对于依赖JavaScript渲染的网站，Scrapy可以结合Selenium使用：

```python
from scrapy import Spider
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

class SeleniumSpider(Spider):
    name = 'selenium_spider'
    start_urls = ['https://example.com/js-page']
    
    def __init__(self):
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        self.driver = webdriver.Chrome(options=chrome_options)
    
    def parse(self, response):
        self.driver.get(response.url)
        # 等待JavaScript执行完成
        self.driver.implicitly_wait(10)
        
        # 使用Selenium操作页面
        # ...
        
        # 关闭浏览器
        self.driver.quit()
```

## 总结

本章介绍了Scrapy框架的基本用法和高级特性。Scrapy提供了一套完整的爬虫解决方案，适合开发大规模、分布式的爬虫项目。通过学习Scrapy，你可以：

1. 结构化地组织爬虫代码
2. 高效地抓取和处理大量网页
3. 灵活地配置和扩展爬虫功能
4. 实现分布式爬取和断点续爬

在下一章中，我们将学习如何爬取动态网页，处理JavaScript渲染的内容。 